{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import utils\n",
    "import USAD\n",
    "\n",
    "import numpy as np\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.1.0\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\") # 1.12.1 이상\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\") # True 여야 합니다.\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\") # True 여야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_default_device()\n",
    "# device = torch.device(torch.device(\"mps:0\") if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './경진대회용 열처리 품질보증 데이터셋/'\n",
    "df = pd.read_csv(path+'data.csv', encoding='cp949')\n",
    "df['TAG_MIN']=pd.to_datetime(df.TAG_MIN, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "dfd = df.describe()\n",
    "dfd.loc['upper_outlier'] = dfd.loc['75%'] + ((dfd.loc['75%'] - dfd.loc['25%'])*1.5)\n",
    "dfd.loc['lower_outlier'] = dfd.loc['25%'] - ((dfd.loc['75%'] - dfd.loc['25%'])*1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측치 처리\n",
    "##### 초당 샘플링 하기때문에 결측치 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outlier(df, dfd):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove('TAG_MIN')\n",
    "    columns.remove('배정번호')\n",
    "\n",
    "    buff_df1 = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for column in columns:\n",
    "        buff_df1[column+'_error'] = 0\n",
    "        buff_df1[column+'_error'] += ((df[column] > dfd.loc['upper_outlier', column]) | (df[column] < dfd.loc['lower_outlier', column])) * 1.0\n",
    "\n",
    "    continue_num = 5\n",
    "\n",
    "    buff_df1 = buff_df1.rolling(window=continue_num).mean()\n",
    "    buff_df1 = (buff_df1>=1)*1.0\n",
    "\n",
    "    df = pd.concat([df, buff_df1], axis=1)\n",
    "    df.drop(index=range(continue_num), inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    buff_df2 = pd.DataFrame(index=df.index)\n",
    "\n",
    "    preocesses = ['건조', '세정', '소입', '솔트']\n",
    "\n",
    "    for process in preocesses:\n",
    "        buff_df2[process+'_error'] = df[[column for column in df.columns if process in column and 'error' in column]].sum(axis=1)\n",
    "\n",
    "    df.drop(columns=[column for column in df.columns if 'error' in column], inplace=True)\n",
    "    df = pd.concat([df, buff_df2], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = count_outlier(df, dfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['TAG_MIN', '배정번호'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>건조 1존 OP</th>\n",
       "      <th>건조 2존 OP</th>\n",
       "      <th>건조로 온도 1 Zone</th>\n",
       "      <th>건조로 온도 2 Zone</th>\n",
       "      <th>세정기</th>\n",
       "      <th>소입1존 OP</th>\n",
       "      <th>소입2존 OP</th>\n",
       "      <th>소입3존 OP</th>\n",
       "      <th>소입4존 OP</th>\n",
       "      <th>소입로 CP 값</th>\n",
       "      <th>...</th>\n",
       "      <th>소입로 온도 3 Zone</th>\n",
       "      <th>소입로 온도 4 Zone</th>\n",
       "      <th>솔트 컨베이어 온도 1 Zone</th>\n",
       "      <th>솔트 컨베이어 온도 2 Zone</th>\n",
       "      <th>솔트조 온도 1 Zone</th>\n",
       "      <th>솔트조 온도 2 Zone</th>\n",
       "      <th>건조_error</th>\n",
       "      <th>세정_error</th>\n",
       "      <th>소입_error</th>\n",
       "      <th>솔트_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.9607</td>\n",
       "      <td>29.596700</td>\n",
       "      <td>98.7825</td>\n",
       "      <td>99.2001</td>\n",
       "      <td>68.4275</td>\n",
       "      <td>75.6285</td>\n",
       "      <td>61.1314</td>\n",
       "      <td>51.8036</td>\n",
       "      <td>71.6125</td>\n",
       "      <td>0.450566</td>\n",
       "      <td>...</td>\n",
       "      <td>859.991</td>\n",
       "      <td>859.731</td>\n",
       "      <td>294.721</td>\n",
       "      <td>272.599</td>\n",
       "      <td>328.740</td>\n",
       "      <td>328.869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.2665</td>\n",
       "      <td>32.172600</td>\n",
       "      <td>98.7825</td>\n",
       "      <td>99.2001</td>\n",
       "      <td>68.4889</td>\n",
       "      <td>70.9577</td>\n",
       "      <td>59.6177</td>\n",
       "      <td>51.8148</td>\n",
       "      <td>71.5936</td>\n",
       "      <td>0.450445</td>\n",
       "      <td>...</td>\n",
       "      <td>859.991</td>\n",
       "      <td>859.731</td>\n",
       "      <td>294.781</td>\n",
       "      <td>272.661</td>\n",
       "      <td>328.740</td>\n",
       "      <td>328.869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.2216</td>\n",
       "      <td>29.828000</td>\n",
       "      <td>98.7825</td>\n",
       "      <td>99.2001</td>\n",
       "      <td>68.4275</td>\n",
       "      <td>75.5710</td>\n",
       "      <td>59.6774</td>\n",
       "      <td>51.8251</td>\n",
       "      <td>70.1123</td>\n",
       "      <td>0.450361</td>\n",
       "      <td>...</td>\n",
       "      <td>859.991</td>\n",
       "      <td>859.793</td>\n",
       "      <td>294.781</td>\n",
       "      <td>272.661</td>\n",
       "      <td>328.740</td>\n",
       "      <td>328.869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.1841</td>\n",
       "      <td>29.867600</td>\n",
       "      <td>98.7918</td>\n",
       "      <td>99.1460</td>\n",
       "      <td>68.4999</td>\n",
       "      <td>75.2962</td>\n",
       "      <td>61.1979</td>\n",
       "      <td>51.8345</td>\n",
       "      <td>70.1451</td>\n",
       "      <td>0.450424</td>\n",
       "      <td>...</td>\n",
       "      <td>859.991</td>\n",
       "      <td>859.793</td>\n",
       "      <td>294.781</td>\n",
       "      <td>272.722</td>\n",
       "      <td>328.740</td>\n",
       "      <td>328.929</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.1841</td>\n",
       "      <td>29.867600</td>\n",
       "      <td>98.7918</td>\n",
       "      <td>99.1460</td>\n",
       "      <td>68.4999</td>\n",
       "      <td>75.2962</td>\n",
       "      <td>61.1979</td>\n",
       "      <td>51.8345</td>\n",
       "      <td>70.1451</td>\n",
       "      <td>0.450424</td>\n",
       "      <td>...</td>\n",
       "      <td>859.991</td>\n",
       "      <td>859.793</td>\n",
       "      <td>294.842</td>\n",
       "      <td>272.722</td>\n",
       "      <td>328.800</td>\n",
       "      <td>328.929</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935035</th>\n",
       "      <td>61.2937</td>\n",
       "      <td>0.093966</td>\n",
       "      <td>99.9056</td>\n",
       "      <td>100.4870</td>\n",
       "      <td>67.1140</td>\n",
       "      <td>71.8356</td>\n",
       "      <td>60.7118</td>\n",
       "      <td>50.3777</td>\n",
       "      <td>76.3514</td>\n",
       "      <td>0.447112</td>\n",
       "      <td>...</td>\n",
       "      <td>859.930</td>\n",
       "      <td>859.419</td>\n",
       "      <td>280.798</td>\n",
       "      <td>272.102</td>\n",
       "      <td>332.058</td>\n",
       "      <td>332.247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935036</th>\n",
       "      <td>61.7260</td>\n",
       "      <td>0.283887</td>\n",
       "      <td>99.8440</td>\n",
       "      <td>100.4870</td>\n",
       "      <td>67.1140</td>\n",
       "      <td>77.1783</td>\n",
       "      <td>60.6722</td>\n",
       "      <td>55.0392</td>\n",
       "      <td>76.3017</td>\n",
       "      <td>0.447982</td>\n",
       "      <td>...</td>\n",
       "      <td>859.743</td>\n",
       "      <td>859.419</td>\n",
       "      <td>280.859</td>\n",
       "      <td>272.163</td>\n",
       "      <td>332.058</td>\n",
       "      <td>332.247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935037</th>\n",
       "      <td>61.6784</td>\n",
       "      <td>0.205745</td>\n",
       "      <td>99.7825</td>\n",
       "      <td>100.4870</td>\n",
       "      <td>67.1140</td>\n",
       "      <td>73.1729</td>\n",
       "      <td>62.1574</td>\n",
       "      <td>56.4989</td>\n",
       "      <td>76.2566</td>\n",
       "      <td>0.448688</td>\n",
       "      <td>...</td>\n",
       "      <td>859.681</td>\n",
       "      <td>859.419</td>\n",
       "      <td>280.859</td>\n",
       "      <td>272.163</td>\n",
       "      <td>332.058</td>\n",
       "      <td>332.186</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935038</th>\n",
       "      <td>61.5148</td>\n",
       "      <td>0.136414</td>\n",
       "      <td>99.8440</td>\n",
       "      <td>100.4870</td>\n",
       "      <td>67.1140</td>\n",
       "      <td>73.8443</td>\n",
       "      <td>62.0722</td>\n",
       "      <td>54.8139</td>\n",
       "      <td>76.2158</td>\n",
       "      <td>0.448501</td>\n",
       "      <td>...</td>\n",
       "      <td>859.743</td>\n",
       "      <td>859.419</td>\n",
       "      <td>280.921</td>\n",
       "      <td>272.163</td>\n",
       "      <td>332.058</td>\n",
       "      <td>332.186</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935039</th>\n",
       "      <td>61.4936</td>\n",
       "      <td>0.074816</td>\n",
       "      <td>99.7825</td>\n",
       "      <td>100.5490</td>\n",
       "      <td>67.1754</td>\n",
       "      <td>74.4613</td>\n",
       "      <td>60.4183</td>\n",
       "      <td>54.7721</td>\n",
       "      <td>76.1788</td>\n",
       "      <td>0.448735</td>\n",
       "      <td>...</td>\n",
       "      <td>859.743</td>\n",
       "      <td>859.419</td>\n",
       "      <td>280.982</td>\n",
       "      <td>272.163</td>\n",
       "      <td>332.058</td>\n",
       "      <td>332.186</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2935040 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         건조 1존 OP   건조 2존 OP  건조로 온도 1 Zone  건조로 온도 2 Zone      세정기  소입1존 OP  소입2존 OP  소입3존 OP  소입4존 OP  소입로 CP 값  ...  소입로 온도 3 Zone  소입로 온도 4 Zone  솔트 컨베이어 온도 1 Zone  솔트 컨베이어 온도 2 Zone  솔트조 온도 1 Zone  솔트조 온도 2 Zone  건조_error  세정_error  소입_error  솔트_error\n",
       "0         75.9607  29.596700        98.7825        99.2001  68.4275  75.6285  61.1314  51.8036  71.6125  0.450566  ...        859.991        859.731            294.721            272.599        328.740        328.869       2.0       0.0       0.0       2.0\n",
       "1         76.2665  32.172600        98.7825        99.2001  68.4889  70.9577  59.6177  51.8148  71.5936  0.450445  ...        859.991        859.731            294.781            272.661        328.740        328.869       2.0       0.0       0.0       2.0\n",
       "2         76.2216  29.828000        98.7825        99.2001  68.4275  75.5710  59.6774  51.8251  70.1123  0.450361  ...        859.991        859.793            294.781            272.661        328.740        328.869       2.0       0.0       0.0       2.0\n",
       "3         76.1841  29.867600        98.7918        99.1460  68.4999  75.2962  61.1979  51.8345  70.1451  0.450424  ...        859.991        859.793            294.781            272.722        328.740        328.929       2.0       0.0       0.0       2.0\n",
       "4         76.1841  29.867600        98.7918        99.1460  68.4999  75.2962  61.1979  51.8345  70.1451  0.450424  ...        859.991        859.793            294.842            272.722        328.800        328.929       2.0       0.0       0.0       2.0\n",
       "...           ...        ...            ...            ...      ...      ...      ...      ...      ...       ...  ...            ...            ...                ...                ...            ...            ...       ...       ...       ...       ...\n",
       "2935035   61.2937   0.093966        99.9056       100.4870  67.1140  71.8356  60.7118  50.3777  76.3514  0.447112  ...        859.930        859.419            280.798            272.102        332.058        332.247       2.0       0.0       0.0       0.0\n",
       "2935036   61.7260   0.283887        99.8440       100.4870  67.1140  77.1783  60.6722  55.0392  76.3017  0.447982  ...        859.743        859.419            280.859            272.163        332.058        332.247       2.0       0.0       0.0       0.0\n",
       "2935037   61.6784   0.205745        99.7825       100.4870  67.1140  73.1729  62.1574  56.4989  76.2566  0.448688  ...        859.681        859.419            280.859            272.163        332.058        332.186       2.0       0.0       0.0       0.0\n",
       "2935038   61.5148   0.136414        99.8440       100.4870  67.1140  73.8443  62.0722  54.8139  76.2158  0.448501  ...        859.743        859.419            280.921            272.163        332.058        332.186       2.0       0.0       0.0       0.0\n",
       "2935039   61.4936   0.074816        99.7825       100.5490  67.1754  74.4613  60.4183  54.7721  76.1788  0.448735  ...        859.743        859.419            280.982            272.163        332.058        332.186       2.0       0.0       0.0       0.0\n",
       "\n",
       "[2935040 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drying_df = df[[column for column in df.columns if '건조'  in column]]\n",
    "cleaning_df =df[[column for column in df.columns if '세정'  in column]]\n",
    "quenching_df = df[[column for column in df.columns if '소입'  in column]]\n",
    "salt_df = df[[column for column in df.columns if '솔트'  in column]]\n",
    "\n",
    "drying_normal = drying_df[drying_df['건조_error']==0].copy()\n",
    "drying_abnormal = drying_df[drying_df['건조_error']>0].copy()\n",
    "cleaning_normal = cleaning_df[cleaning_df['세정_error']==0].copy()\n",
    "cleaning_abnormal = cleaning_df[cleaning_df['세정_error']>0].copy()\n",
    "quenching_normal = quenching_df[quenching_df['소입_error']==0].copy()\n",
    "quenching_abnormal = quenching_df[quenching_df['소입_error']>0].copy()\n",
    "salt_normal = salt_df[salt_df['솔트_error']==0].copy()\n",
    "salt_abnormal = salt_df[salt_df['솔트_error']>0].copy()\n",
    "\n",
    "df_list = [drying_normal, drying_abnormal, cleaning_normal, cleaning_abnormal, quenching_normal, quenching_abnormal, salt_normal, salt_abnormal]\n",
    "\n",
    "for idx, df in enumerate(df_list):\n",
    "    df_list[idx].drop(columns=[column for column in df.columns if 'error' in column], inplace=True)\n",
    "    df_list[idx].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drying_scaler = preprocessing.MinMaxScaler()\n",
    "cleaning_scaler = preprocessing.MinMaxScaler()\n",
    "quenching_scaler = preprocessing.MinMaxScaler()\n",
    "salt_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "scaler_list = [drying_scaler, cleaning_scaler, quenching_scaler, salt_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(df_list):\n",
    "    if not idx%2:\n",
    "        x = df.values\n",
    "        x_scaled = scaler_list[idx//2].fit_transform(x)\n",
    "        df_list[idx] = pd.DataFrame(x_scaled)\n",
    "    elif idx%2:\n",
    "        x = df.values\n",
    "        x_scaled = scaler_list[idx//2].transform(x)\n",
    "        df_list[idx] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_size = 12\n",
    "\n",
    "# window_drying_normal=df_list[0].values[np.arange(window_size)[None, :] + np.arange(df_list[0].shape[0]-window_size)[:, None]] \n",
    "# window_drying_abnormal=df_list[1].values[np.arange(window_size)[None, :] + np.arange(df_list[1].shape[0]-window_size)[:, None]] \n",
    "# window_cleaning_normal=df_list[2].values[np.arange(window_size)[None, :] + np.arange(df_list[2].shape[0]-window_size)[:, None]] \n",
    "# window_cleaning_abnormal=df_list[3].values[np.arange(window_size)[None, :] + np.arange(df_list[3].shape[0]-window_size)[:, None]] \n",
    "# window_quenching_normal=df_list[4].values[np.arange(window_size)[None, :] + np.arange(df_list[4].shape[0]-window_size)[:, None]] \n",
    "# window_quenching_abnormal=df_list[5].values[np.arange(window_size)[None, :] + np.arange(df_list[5].shape[0]-window_size)[:, None]] \n",
    "# window_salt_normal=df_list[6].values[np.arange(window_size)[None, :] + np.arange(df_list[6].shape[0]-window_size)[:, None]] \n",
    "# window_salt_abnormal=df_list[7].values[np.arange(window_size)[None, :] + np.arange(df_list[7].shape[0]-window_size)[:, None]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "\n",
    "window_normal_list = []\n",
    "window_abnormal_list = []\n",
    "\n",
    "for idx, scaled_df in enumerate(df_list):\n",
    "    if not idx%2:\n",
    "        window_normal_list.append(df_list[idx].values[np.arange(window_size)[None, :] + np.arange(df_list[idx].shape[0]-window_size)[:, None]])\n",
    "    elif idx%2:\n",
    "        window_abnormal_list.append(df_list[idx].values[np.arange(window_size)[None, :] + np.arange(df_list[idx].shape[0]-window_size)[:, None]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# N_EPOCHS = 100\n",
    "# hidden_size = 100\n",
    "\n",
    "# # set input size\n",
    "# drying_w_size = window_drying_normal.shape[1] * window_drying_normal.shape[2]\n",
    "# drying_z_size = window_drying_normal.shape[1] * hidden_size\n",
    "# cleaning_w_size = window_cleaning_normal.shape[1] * window_cleaning_normal.shape[2]\n",
    "# cleaning_z_size = window_cleaning_normal.shape[1] * hidden_size\n",
    "# quenching_w_size = window_quenching_normal.shape[1] * window_quenching_normal.shape[2]\n",
    "# quenching_z_size = window_quenching_normal.shape[1] * hidden_size\n",
    "# salt_w_size = window_salt_normal.shape[1] * window_salt_normal.shape[2]\n",
    "# salt_z_size = window_salt_normal.shape[1] * hidden_size\n",
    "\n",
    "# # data split\n",
    "# window_drying_normal_train = window_drying_normal[:int(np.floor(.8 *  window_drying_normal.shape[0]))]\n",
    "# window_drying_normal_val = window_drying_normal[int(np.floor(.8 *  window_drying_normal.shape[0])):int(np.floor(window_drying_normal.shape[0]))]\n",
    "# window_cleaning_normal_train = window_cleaning_normal[:int(np.floor(.8 *  window_cleaning_normal.shape[0]))]\n",
    "# window_cleaning_normal_val = window_cleaning_normal[int(np.floor(.8 *  window_cleaning_normal.shape[0])):int(np.floor(window_cleaning_normal.shape[0]))]\n",
    "# window_quenching_normal_train = window_quenching_normal[:int(np.floor(.8 *  window_quenching_normal.shape[0]))]\n",
    "# window_quenching_normal_val = window_quenching_normal[int(np.floor(.8 *  window_quenching_normal.shape[0])):int(np.floor(window_quenching_normal.shape[0]))]\n",
    "# window_salt_normal_train = window_salt_normal[:int(np.floor(.8 *  window_salt_normal.shape[0]))]\n",
    "# window_salt_normal_val = window_salt_normal[int(np.floor(.8 *  window_salt_normal.shape[0])):int(np.floor(window_salt_normal.shape[0]))]\n",
    "\n",
    "# window_normal_train_list = [window_drying_normal_train, window_cleaning_normal_train, window_quenching_normal_train, window_salt_normal_train]\n",
    "# window_normal_val_list = [window_drying_normal_val, window_cleaning_normal_val, window_quenching_normal_val, window_salt_normal_val]\n",
    "# window_abnormal_test_list = [window_drying_abnormal, window_cleaning_abnormal, window_quenching_abnormal, window_salt_abnormal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 7919\n",
    "N_EPOCHS = 100\n",
    "hidden_size = 100\n",
    "\n",
    "w_size_list = []\n",
    "z_size_list = []\n",
    "\n",
    "for window_normal in window_normal_list:\n",
    "    w_size_list.append(window_normal.shape[1] * window_normal.shape[2])\n",
    "    z_size_list.append(window_normal.shape[1] * hidden_size)\n",
    "\n",
    "# Data Split\n",
    "window_normal_train_list = []\n",
    "window_normal_val_list = []\n",
    "window_abnormal_test_list = window_abnormal_list.copy()\n",
    "\n",
    "for window_normal in window_normal_list:\n",
    "    window_normal_train_list.append(window_normal[:int(np.floor(.8 *  window_normal.shape[0]))])\n",
    "    window_normal_val_list.append(window_normal[int(np.floor(.8 *  window_normal.shape[0])):int(np.floor(window_normal.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "\n",
    "train_loader_list = []\n",
    "val_loader_list = []\n",
    "test_loader_list = []\n",
    "\n",
    "for idx, window_normal_train in enumerate(window_normal_train_list):\n",
    "    train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "        torch.from_numpy(window_normal_train).float().view((window_normal_train.shape[0], w_size_list[idx]))\n",
    "    ), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    train_loader_list.append(train_loader)\n",
    "\n",
    "for idx, window_normal_val in enumerate(window_normal_val_list):\n",
    "    val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "        torch.from_numpy(window_normal_val).float().view(window_normal_val.shape[0], w_size_list[idx])\n",
    "    ), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    val_loader_list.append(val_loader)\n",
    "\n",
    "for idx, window_abnormal_test in enumerate(window_abnormal_test_list):\n",
    "    test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "        torch.from_numpy(window_abnormal_test).float().view(window_abnormal_test.shape[0], w_size_list[idx])\n",
    "    ), batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    test_loader_list.append(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "drying_model = USAD.UsadModel(w_size_list[0], z_size_list[0])\n",
    "drying_model = utils.to_device(drying_model, device)\n",
    "cleaning_model = USAD.UsadModel(w_size_list[1], z_size_list[1])\n",
    "cleaning_model = utils.to_device(cleaning_model, device)\n",
    "quenching_model = USAD.UsadModel(w_size_list[2], z_size_list[2])\n",
    "quenching_model = utils.to_device(quenching_model, device)\n",
    "salt_model = USAD.UsadModel(w_size_list[3], z_size_list[3])\n",
    "salt_model = utils.to_device(salt_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/leebyeonghwa/studyspace/KAMP/USADforPredictMaintnance.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leebyeonghwa/studyspace/KAMP/USADforPredictMaintnance.ipynb#Y116sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m drying_history \u001b[39m=\u001b[39m USAD\u001b[39m.\u001b[39;49mtraining(N_EPOCHS, drying_model, train_loader_list[\u001b[39m0\u001b[39;49m], val_loader_list[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/studyspace/KAMP/USAD.py:117\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(epochs, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m    114\u001b[0m optimizer1\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    116\u001b[0m \u001b[39m# train AE2\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m loss1, loss2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(batch, epoch\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    118\u001b[0m loss2\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    119\u001b[0m optimizer2\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/studyspace/KAMP/USAD.py:63\u001b[0m, in \u001b[0;36mUsadModel.train_step\u001b[0;34m(self, batch, n)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, batch, n):\n\u001b[0;32m---> 63\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(batch)\n\u001b[1;32m     64\u001b[0m     w1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder1(z)\n\u001b[1;32m     65\u001b[0m     w2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder2(z)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/studyspace/KAMP/USAD.py:25\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m     23\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(out)\n\u001b[1;32m     24\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m---> 25\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear3(out)\n\u001b[1;32m     26\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "drying_history = USAD.training(N_EPOCHS, drying_model, train_loader_list[0], val_loader_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7919x12 and 48x24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/leebyeonghwa/studyspace/KAMP/USADforPredictMaintnance.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leebyeonghwa/studyspace/KAMP/USADforPredictMaintnance.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cleaning_history \u001b[39m=\u001b[39m USAD\u001b[39m.\u001b[39;49mtraining(N_EPOCHS, drying_model, train_loader_list[\u001b[39m1\u001b[39;49m], val_loader_list[\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[0;32m~/studyspace/KAMP/USAD.py:111\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(epochs, model, train_loader, val_loader, opt_func)\u001b[0m\n\u001b[1;32m    108\u001b[0m batch\u001b[39m=\u001b[39mutils\u001b[39m.\u001b[39mto_device(batch, device)\n\u001b[1;32m    110\u001b[0m \u001b[39m# train AE1\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m loss1, loss2 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(batch, epoch\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    112\u001b[0m loss1\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    113\u001b[0m optimizer1\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/studyspace/KAMP/USAD.py:63\u001b[0m, in \u001b[0;36mUsadModel.train_step\u001b[0;34m(self, batch, n)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(\u001b[39mself\u001b[39m, batch, n):\n\u001b[0;32m---> 63\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(batch)\n\u001b[1;32m     64\u001b[0m     w1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder1(z)\n\u001b[1;32m     65\u001b[0m     w2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder2(z)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/studyspace/KAMP/USAD.py:21\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, w):\n\u001b[0;32m---> 21\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(w)\n\u001b[1;32m     22\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     23\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(out)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7919x12 and 48x24)"
     ]
    }
   ],
   "source": [
    "cleaning_history = USAD.training(N_EPOCHS, cleaning_model, train_loader_list[1], val_loader_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss1: 0.0041, val_loss2: 0.0044\n",
      "Epoch [1], val_loss1: 0.0038, val_loss2: 0.0004\n",
      "Epoch [2], val_loss1: 0.0041, val_loss2: -0.0006\n",
      "Epoch [3], val_loss1: 0.0021, val_loss2: -0.0001\n",
      "Epoch [4], val_loss1: 0.0020, val_loss2: -0.0003\n",
      "Epoch [5], val_loss1: 0.0016, val_loss2: -0.0002\n",
      "Epoch [6], val_loss1: 0.0011, val_loss2: -0.0001\n",
      "Epoch [7], val_loss1: 0.0009, val_loss2: -0.0000\n",
      "Epoch [8], val_loss1: 0.0012, val_loss2: -0.0002\n",
      "Epoch [9], val_loss1: 0.0008, val_loss2: -0.0000\n",
      "Epoch [10], val_loss1: 0.0007, val_loss2: -0.0001\n",
      "Epoch [11], val_loss1: 0.0008, val_loss2: -0.0002\n",
      "Epoch [12], val_loss1: 0.0006, val_loss2: -0.0000\n",
      "Epoch [13], val_loss1: 0.0006, val_loss2: -0.0000\n",
      "Epoch [14], val_loss1: 0.0006, val_loss2: -0.0001\n",
      "Epoch [15], val_loss1: 0.0007, val_loss2: -0.0001\n",
      "Epoch [16], val_loss1: 0.0004, val_loss2: -0.0000\n",
      "Epoch [17], val_loss1: 0.0006, val_loss2: -0.0001\n",
      "Epoch [18], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [19], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [20], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [21], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [22], val_loss1: 0.0008, val_loss2: -0.0002\n",
      "Epoch [23], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [24], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [25], val_loss1: 0.0003, val_loss2: -0.0000\n",
      "Epoch [26], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [27], val_loss1: 0.0003, val_loss2: -0.0000\n",
      "Epoch [28], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [29], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [30], val_loss1: 0.0005, val_loss2: -0.0002\n",
      "Epoch [31], val_loss1: 0.0003, val_loss2: -0.0000\n",
      "Epoch [32], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [33], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [34], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [35], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [36], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [37], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [38], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [39], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [40], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [41], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [42], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [43], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [44], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [45], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [46], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [47], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [48], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [49], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [50], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [51], val_loss1: 0.0002, val_loss2: -0.0001\n",
      "Epoch [52], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [53], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [54], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [55], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [56], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [57], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [58], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [59], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [60], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [61], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [62], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [63], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [64], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [65], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [66], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [67], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [68], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [69], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [70], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [71], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [72], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [73], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [74], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [75], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [76], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [77], val_loss1: 0.0002, val_loss2: -0.0001\n",
      "Epoch [78], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [79], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [80], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [81], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [82], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [83], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [84], val_loss1: 0.0002, val_loss2: -0.0000\n",
      "Epoch [85], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [86], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [87], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [88], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [89], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [90], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [91], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [92], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [93], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [94], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [95], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [96], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [97], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [98], val_loss1: 0.0001, val_loss2: -0.0000\n",
      "Epoch [99], val_loss1: 0.0001, val_loss2: -0.0000\n"
     ]
    }
   ],
   "source": [
    "quenching_history = USAD.training(N_EPOCHS, quenching_model, train_loader_list[2], val_loader_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss1: 0.0123, val_loss2: 0.0134\n",
      "Epoch [1], val_loss1: 0.0114, val_loss2: 0.0023\n",
      "Epoch [2], val_loss1: 0.0084, val_loss2: 0.0008\n",
      "Epoch [3], val_loss1: 0.0111, val_loss2: -0.0014\n",
      "Epoch [4], val_loss1: 0.0057, val_loss2: -0.0005\n",
      "Epoch [5], val_loss1: 0.0049, val_loss2: -0.0007\n",
      "Epoch [6], val_loss1: 0.0049, val_loss2: -0.0011\n",
      "Epoch [7], val_loss1: 0.0044, val_loss2: -0.0008\n",
      "Epoch [8], val_loss1: 0.0046, val_loss2: -0.0011\n",
      "Epoch [9], val_loss1: 0.0034, val_loss2: -0.0007\n",
      "Epoch [10], val_loss1: 0.0030, val_loss2: -0.0006\n",
      "Epoch [11], val_loss1: 0.0026, val_loss2: -0.0005\n",
      "Epoch [12], val_loss1: 0.0024, val_loss2: -0.0004\n",
      "Epoch [13], val_loss1: 0.0028, val_loss2: -0.0007\n",
      "Epoch [14], val_loss1: 0.0020, val_loss2: -0.0004\n",
      "Epoch [15], val_loss1: 0.0023, val_loss2: -0.0006\n",
      "Epoch [16], val_loss1: 0.0018, val_loss2: -0.0004\n",
      "Epoch [17], val_loss1: 0.0020, val_loss2: -0.0005\n",
      "Epoch [18], val_loss1: 0.0018, val_loss2: -0.0004\n",
      "Epoch [19], val_loss1: 0.0015, val_loss2: -0.0003\n",
      "Epoch [20], val_loss1: 0.0015, val_loss2: -0.0004\n",
      "Epoch [21], val_loss1: 0.0013, val_loss2: -0.0003\n",
      "Epoch [22], val_loss1: 0.0013, val_loss2: -0.0003\n",
      "Epoch [23], val_loss1: 0.0012, val_loss2: -0.0003\n",
      "Epoch [24], val_loss1: 0.0013, val_loss2: -0.0003\n",
      "Epoch [25], val_loss1: 0.0011, val_loss2: -0.0002\n",
      "Epoch [26], val_loss1: 0.0011, val_loss2: -0.0003\n",
      "Epoch [27], val_loss1: 0.0011, val_loss2: -0.0003\n",
      "Epoch [28], val_loss1: 0.0010, val_loss2: -0.0003\n",
      "Epoch [29], val_loss1: 0.0010, val_loss2: -0.0002\n",
      "Epoch [30], val_loss1: 0.0010, val_loss2: -0.0003\n",
      "Epoch [31], val_loss1: 0.0010, val_loss2: -0.0003\n",
      "Epoch [32], val_loss1: 0.0009, val_loss2: -0.0002\n",
      "Epoch [33], val_loss1: 0.0009, val_loss2: -0.0002\n",
      "Epoch [34], val_loss1: 0.0008, val_loss2: -0.0002\n",
      "Epoch [35], val_loss1: 0.0008, val_loss2: -0.0002\n",
      "Epoch [36], val_loss1: 0.0009, val_loss2: -0.0003\n",
      "Epoch [37], val_loss1: 0.0007, val_loss2: -0.0002\n",
      "Epoch [38], val_loss1: 0.0008, val_loss2: -0.0002\n",
      "Epoch [39], val_loss1: 0.0007, val_loss2: -0.0002\n",
      "Epoch [40], val_loss1: 0.0007, val_loss2: -0.0002\n",
      "Epoch [41], val_loss1: 0.0007, val_loss2: -0.0002\n",
      "Epoch [42], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [43], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [44], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [45], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [46], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [47], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [48], val_loss1: 0.0006, val_loss2: -0.0002\n",
      "Epoch [49], val_loss1: 0.0006, val_loss2: -0.0001\n",
      "Epoch [50], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [51], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [52], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [53], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [54], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [55], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [56], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [57], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [58], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [59], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [60], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [61], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [62], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [63], val_loss1: 0.0005, val_loss2: -0.0001\n",
      "Epoch [64], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [65], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [66], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [67], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [68], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [69], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [70], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [71], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [72], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [73], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [74], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [75], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [76], val_loss1: 0.0004, val_loss2: -0.0001\n",
      "Epoch [77], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [78], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [79], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [80], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [81], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [82], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [83], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [84], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [85], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [86], val_loss1: 0.0003, val_loss2: -0.0001\n",
      "Epoch [87], val_loss1: 0.0003, val_loss2: -0.0001\n"
     ]
    }
   ],
   "source": [
    "salt_history = USAD.training(N_EPOCHS, salt_model, train_loader_list[3], val_loader_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Line2D.set() got an unexpected keyword argument 'lable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/leebyeonghwa/studyspace/KAMP/USADforPredictMaintnance.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leebyeonghwa/studyspace/KAMP/USADforPredictMaintnance.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m utils\u001b[39m.\u001b[39;49mplot_history(drying_history)\n",
      "File \u001b[0;32m~/studyspace/KAMP/utils.py:28\u001b[0m, in \u001b[0;36mplot_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     25\u001b[0m losses2 \u001b[39m=\u001b[39m [x[\u001b[39m'\u001b[39m\u001b[39mval_loss2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m history]\n\u001b[1;32m     27\u001b[0m plt\u001b[39m.\u001b[39mplot(losses1, \u001b[39m'\u001b[39m\u001b[39m-x\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m plt\u001b[39m.\u001b[39;49mplot(losses2, \u001b[39m'\u001b[39;49m\u001b[39m-x\u001b[39;49m\u001b[39m'\u001b[39;49m, lable\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mloss2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[39m*\u001b[39margs: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m ArrayLike \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   3579\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m   3580\u001b[0m         scalex\u001b[39m=\u001b[39;49mscalex,\n\u001b[1;32m   3581\u001b[0m         scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   3582\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   3583\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3584\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/axes/_base.py:539\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(result)\n\u001b[1;32m    538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m [l[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m l \u001b[39min\u001b[39;49;00m result]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/axes/_base.py:539\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(result)\n\u001b[1;32m    538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m [l[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m l \u001b[39min\u001b[39;49;00m result]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/axes/_base.py:532\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     labels \u001b[39m=\u001b[39m [label] \u001b[39m*\u001b[39m n_datasets\n\u001b[0;32m--> 532\u001b[0m result \u001b[39m=\u001b[39m (make_artist(axes, x[:, j \u001b[39m%\u001b[39;49m ncx], y[:, j \u001b[39m%\u001b[39;49m ncy], kw,\n\u001b[1;32m    533\u001b[0m                       {\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, \u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m: label})\n\u001b[1;32m    534\u001b[0m           \u001b[39mfor\u001b[39;00m j, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(labels))\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m return_kwargs:\n\u001b[1;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(result)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/axes/_base.py:346\u001b[0m, in \u001b[0;36m_process_plot_var_args._makeline\u001b[0;34m(self, axes, x, y, kw, kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m default_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getdefaults(\u001b[39mset\u001b[39m(), kw)\n\u001b[1;32m    345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setdefaults(default_dict, kw)\n\u001b[0;32m--> 346\u001b[0m seg \u001b[39m=\u001b[39m mlines\u001b[39m.\u001b[39;49mLine2D(x, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m seg, kw\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/lines.py:407\u001b[0m, in \u001b[0;36mLine2D.__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, gapcolor, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_markeredgewidth(markeredgewidth)\n\u001b[1;32m    405\u001b[0m \u001b[39m# update kwargs before updating data to give the caller a\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39m# chance to init axes (and hence unit support)\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_update(kwargs)\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpickradius \u001b[39m=\u001b[39m pickradius\n\u001b[1;32m    409\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mind_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/artist.py:1219\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_update\u001b[39m(\u001b[39mself\u001b[39m, kwargs):\n\u001b[1;32m   1213\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \n\u001b[1;32m   1217\u001b[0m \u001b[39m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_props(\n\u001b[1;32m   1220\u001b[0m         kwargs, \u001b[39m\"\u001b[39;49m\u001b[39m{cls.__name__}\u001b[39;49;00m\u001b[39m.set() got an unexpected keyword argument \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1221\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m{prop_name!r}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/KAMP-JyWOzC9O/lib/python3.11/site-packages/matplotlib/artist.py:1193\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mset_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1192\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(func):\n\u001b[0;32m-> 1193\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1194\u001b[0m                     errfmt\u001b[39m.\u001b[39mformat(\u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), prop_name\u001b[39m=\u001b[39mk))\n\u001b[1;32m   1195\u001b[0m             ret\u001b[39m.\u001b[39mappend(func(v))\n\u001b[1;32m   1196\u001b[0m \u001b[39mif\u001b[39;00m ret:\n",
      "\u001b[0;31mAttributeError\u001b[0m: Line2D.set() got an unexpected keyword argument 'lable'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_history(drying_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(cleaning_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(quenching_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(salt_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [drying_model, cleaning_model, quenching_model, salt_model]\n",
    "model_name_list = ['건조', '세정', '소입', '솔트']\n",
    "\n",
    "for name, model in zip(model_name_list, model_list):\n",
    "    torch.save({\n",
    "                'encoder': model.encoder.state_dict(),\n",
    "                'decoder1': model.decoder1.state_dict(),\n",
    "                'decoder2': model.decoder2.state_dict()\n",
    "                }, \"./model/\"+name+\"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model_name in enumerate(model_name_list):\n",
    "    checkpoint = torch.load(\"model.pth\")\n",
    "    model_list[idx].encoder.load_state_dict(checkpoint['encoder'])\n",
    "    model_list[idx].decoder1.load_state_dict(checkpoint['decoder1'])\n",
    "    model_list[idx].decoder2.load_state_dict(checkpoint['decoder2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drying_result = USAD.testing(model_list[0], test_loader_list[0])\n",
    "cleaning_result = USAD.testing(model_list[1], test_loader_list[1])\n",
    "quenching_result = USAD.testing(model_list[2], test_loader_list[2])\n",
    "salt_result = USAD.testing(model_list[3], test_loader_list[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KAMP-JyWOzC9O",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
